# Curi1 World æ™ºèƒ½ä½“æ¥å£æ€»ç»“

## ğŸ¯ æ¥å£ä½ç½®

### **ä¸»è¦æ¥å£æ–‡ä»¶**

| æ–‡ä»¶è·¯å¾„ | è¯´æ˜ | è§’è‰² |
|---------|------|------|
| [src/agents/base.py](src/agents/base.py) | æŠ½è±¡åŸºç±» | å®šä¹‰ç»Ÿä¸€API |
| [src/agents/dreamer_agent.py](src/agents/dreamer_agent.py) | DreamerV4å®ç° | **å¼ºåŒ–å­¦ä¹ ** |
| [src/agents/bc_agent.py](src/agents/bc_agent.py) | BCå®ç° | **æ¨¡ä»¿å­¦ä¹ ** |
| [src/agents/factory.py](src/agents/factory.py) | å·¥å‚å‡½æ•° | åˆ›å»ºå’ŒåŠ è½½ |

---

## ğŸ“¦ å¿«é€Ÿä½¿ç”¨

### **1. å¼ºåŒ–å­¦ä¹ ï¼ˆDreamerV4ï¼‰**

```python
from src.agents import make_agent, get_default_config

# åˆ›å»ºæ™ºèƒ½ä½“
config = get_default_config('dreamer')
agent = make_agent('dreamer', config)

# ç¯å¢ƒäº¤äº’
obs = env.reset()
agent.reset()  # é‡ç½®RSSMçŠ¶æ€

for step in range(1000):
    # é€‰æ‹©åŠ¨ä½œ
    action = agent.select_action(obs, deterministic=False)
    obs, reward, done, _ = env.step(action)

    if done:
        agent.reset()
        obs = env.reset()

# è®­ç»ƒ
batch = {
    'observations': torch.Tensor,  # [B, T, C, H, W]
    'actions': torch.Tensor,       # [B, T, action_dim]
    'rewards': torch.Tensor,       # [B, T]
    'dones': torch.Tensor          # [B, T]
}
losses = agent.train_step(batch)
```

### **2. æ¨¡ä»¿å­¦ä¹ ï¼ˆBCï¼‰**

```python
from src.agents import make_agent, get_default_config

# åˆ›å»ºæ™ºèƒ½ä½“
config = get_default_config('bc')
agent = make_agent('bc', config)

# ä»ä¸“å®¶æ¼”ç¤ºè®­ç»ƒ
demo_batch = {
    'observations': torch.Tensor,  # [B, C, H, W]
    'actions': torch.Tensor        # [B, action_dim]
}
losses = agent.train_step(demo_batch)

# æ¨ç†
obs = env.reset()
action = agent.select_action(obs, deterministic=True)
```

---

## ğŸ”Œ ç»Ÿä¸€æ¥å£

### **BaseAgent åŸºç±»**

æ‰€æœ‰æ™ºèƒ½ä½“éƒ½å¿…é¡»å®ç°ä»¥ä¸‹æ–¹æ³•ï¼š

```python
class BaseAgent(ABC):
    # === æ ¸å¿ƒæ–¹æ³• ===
    def reset(self):
        """é‡ç½®æ™ºèƒ½ä½“çŠ¶æ€"""

    def select_action(
        self,
        observation: np.ndarray | torch.Tensor,
        deterministic: bool = False
    ) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œ [action_dim]"""

    def predict_action_chunk(
        self,
        observation: np.ndarray | torch.Tensor,
        chunk_size: int = 1
    ) -> np.ndarray:
        """é¢„æµ‹åŠ¨ä½œåºåˆ— [chunk_size, action_dim]"""

    def train_step(
        self,
        batch: Dict[str, torch.Tensor]
    ) -> Dict[str, float]:
        """è®­ç»ƒæ­¥éª¤ï¼Œè¿”å›æŸå¤±å­—å…¸"""

    # === æ¨¡å‹ç®¡ç† ===
    def save(self, save_path: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""

    @classmethod
    def load(cls, load_path: str, **kwargs):
        """åŠ è½½æ£€æŸ¥ç‚¹"""

    def state_dict(self) -> Dict:
        """è·å–çŠ¶æ€å­—å…¸"""

    def load_state_dict(self, state_dict: Dict):
        """åŠ è½½çŠ¶æ€å­—å…¸"""

    def eval(self):
        """è¯„ä¼°æ¨¡å¼"""

    def train(self):
        """è®­ç»ƒæ¨¡å¼"""
```

---

## ğŸ†š ä¸¤ç§æ™ºèƒ½ä½“å¯¹æ¯”

| ç»´åº¦ | **DreamerV4Agent** | **BehaviorCloningAgent** |
|------|-------------------|-------------------------|
| **å­¦ä¹ èŒƒå¼** | å¼ºåŒ–å­¦ä¹  | ç›‘ç£å­¦ä¹ ï¼ˆæ¨¡ä»¿å­¦ä¹ ï¼‰ |
| **æ•°æ®éœ€æ±‚** | ç¯å¢ƒäº¤äº’æ•°æ® + å¥–åŠ± | ä¸“å®¶æ¼”ç¤ºæ•°æ® |
| **è®­ç»ƒä¿¡å·** | Reward | MSEæŸå¤± |
| **æ¢ç´¢** | âœ… æ”¯æŒ | âŒ æ— æ¢ç´¢ |
| **æ ·æœ¬æ•ˆç‡** | é«˜ï¼ˆæ¨¡å‹è¾…åŠ©ï¼‰ | ä¸­ç­‰ |
| **æ³›åŒ–èƒ½åŠ›** | å¯è¶…è¶Šæ¼”ç¤º | å—é™äºæ¼”ç¤º |
| **è®­ç»ƒå¤æ‚åº¦** | é«˜ | ä½ |
| **æ¨ç†é€Ÿåº¦** | å¿« | å¿« |
| **çŠ¶æ€ç®¡ç†** | æœ‰çŠ¶æ€ï¼ˆRSSMï¼‰ | æ— çŠ¶æ€ |

---

## ğŸ—ï¸ æ¥å£è®¾è®¡ç†å¿µ

### **1. å— LeRobot å¯å‘**

```python
# LeRoboté£æ ¼
from lerobot.policies.factory import make_policy

policy = make_policy('sac', config)
action = policy.select_action(obs)

# Curi1 Worldé£æ ¼
from src.agents import make_agent

agent = make_agent('dreamer', config)
action = agent.select_action(obs)
```

**å…³é”®ç›¸ä¼¼ç‚¹ï¼š**
- âœ… å·¥å‚æ¨¡å¼åˆ›å»º
- âœ… ç»Ÿä¸€çš„ `select_action` æ¥å£
- âœ… æ”¯æŒä¿å­˜/åŠ è½½
- âœ… é…ç½®é©±åŠ¨

### **2. æ”¯æŒä¸¤ç§å­¦ä¹ èŒƒå¼**

```python
# RLæ¨¡å¼ï¼šéœ€è¦reward
rl_agent = make_agent('dreamer', config)
batch_rl = {
    'observations': ...,
    'actions': ...,
    'rewards': ...,  # RLéœ€è¦reward
    'dones': ...
}
losses = rl_agent.train_step(batch_rl)

# ILæ¨¡å¼ï¼šåªéœ€è¦(obs, action)å¯¹
il_agent = make_agent('bc', config)
batch_il = {
    'observations': ...,
    'actions': ...  # BCåªéœ€è¦æ¼”ç¤º
}
losses = il_agent.train_step(batch_il)
```

### **3. çµæ´»çš„è§‚å¯Ÿæ ¼å¼**

```python
# æ”¯æŒå¤šç§æ ¼å¼
obs_formats = [
    np.ndarray,  # NumPyæ•°ç»„
    torch.Tensor,  # PyTorchå¼ é‡
    (C, H, W),  # é€šé“ä¼˜å…ˆ
    (H, W, C),  # é€šé“åç½®ï¼ˆè‡ªåŠ¨è½¬æ¢ï¼‰
]

# è‡ªåŠ¨å¤„ç†
action = agent.select_action(obs_any_format)
```

---

## ğŸ“‚ é¡¹ç›®ç»“æ„

```
curi1_world/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ agents/              # ğŸ¯ æ™ºèƒ½ä½“æ¥å£ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”‚   â”œâ”€â”€ __init__.py      # å¯¼å‡ºæ‰€æœ‰æ¥å£
â”‚   â”‚   â”œâ”€â”€ base.py          # BaseAgent æŠ½è±¡åŸºç±»
â”‚   â”‚   â”œâ”€â”€ dreamer_agent.py # DreamerV4 RLå®ç°
â”‚   â”‚   â”œâ”€â”€ bc_agent.py      # BC ILå®ç°
â”‚   â”‚   â””â”€â”€ factory.py       # å·¥å‚å‡½æ•°
â”‚   â”‚
â”‚   â”œâ”€â”€ models/              # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ dreamer.py       # DreamerV4æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ vae.py           # VAEç¼–ç å™¨
â”‚   â”‚   â”œâ”€â”€ rnn.py           # RSSMä¸–ç•Œæ¨¡å‹
â”‚   â”‚   â””â”€â”€ controller.py    # Actor-Critic
â”‚   â”‚
â”‚   â”œâ”€â”€ envs/                # ç¯å¢ƒæ¥å£
â”‚   â”‚   â””â”€â”€ robot_env.py     # Curi1æœºå™¨äººç¯å¢ƒ
â”‚   â”‚
â”‚   â””â”€â”€ utils/               # å·¥å…·æ¨¡å—
â”‚       â”œâ”€â”€ replay_buffer.py
â”‚       â”œâ”€â”€ data_loader.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ scripts/                 # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train_dreamer.py     # RLè®­ç»ƒ
â”‚   â””â”€â”€ evaluate.py          # è¯„ä¼°
â”‚
â”œâ”€â”€ examples/                # ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ agent_usage.py       # å®Œæ•´ç¤ºä¾‹
â”‚
â”œâ”€â”€ docs/                    # æ–‡æ¡£
â”‚   â””â”€â”€ AGENT_API.md         # è¯¦ç»†APIæ–‡æ¡£
â”‚
â””â”€â”€ configs/                 # é…ç½®æ–‡ä»¶
    â”œâ”€â”€ default.yaml
    â”œâ”€â”€ train.yaml
    â””â”€â”€ eval.yaml
```

---

## ğŸ“ ä½¿ç”¨å»ºè®®

### **ä»€ä¹ˆæ—¶å€™ç”¨ DreamerV4ï¼Ÿ**

âœ… **æ¨èåœºæ™¯ï¼š**
- æœ‰æ˜ç¡®çš„å¥–åŠ±ä¿¡å·
- éœ€è¦æ¢ç´¢å’Œä¼˜åŒ–
- æ ·æœ¬æ•ˆç‡å¾ˆé‡è¦
- æƒ³è¶…è¶Šäººç±»æ¼”ç¤º

âŒ **ä¸æ¨èåœºæ™¯ï¼š**
- æ²¡æœ‰å¥–åŠ±ä¿¡å·
- å·²æœ‰å¤§é‡é«˜è´¨é‡æ¼”ç¤º
- éœ€è¦æå¿«é€Ÿéƒ¨ç½²

### **ä»€ä¹ˆæ—¶å€™ç”¨ BCï¼Ÿ**

âœ… **æ¨èåœºæ™¯ï¼š**
- æœ‰å¤§é‡é«˜è´¨é‡æ¼”ç¤º
- æ²¡æœ‰æ˜ç¡®å¥–åŠ±
- éœ€è¦å¿«é€Ÿéƒ¨ç½²
- ä»»åŠ¡ç›¸å¯¹ç®€å•

âŒ **ä¸æ¨èåœºæ™¯ï¼š**
- æ¼”ç¤ºæ•°æ®ä¸è¶³
- éœ€è¦æ¢ç´¢æ–°ç­–ç•¥
- ä»»åŠ¡åˆ†å¸ƒå˜åŒ–å¤§

### **ç»„åˆä½¿ç”¨ç­–ç•¥**

```python
# é˜¶æ®µ1: BCé¢„è®­ç»ƒ
bc_agent = make_agent('bc', bc_config)
# ... ä»æ¼”ç¤ºä¸­å­¦ä¹  ...
bc_agent.save('pretrained_bc.pt')

# é˜¶æ®µ2: RLå¾®è°ƒ
rl_agent = make_agent('dreamer', rl_config)
# å¯é€‰: è¿ç§»BCçš„è§‚å¯Ÿç¼–ç å™¨
# rl_agent.model.vae.encoder.load_state_dict(
#     bc_agent.policy.encoder.state_dict()
# )
# ... RLè®­ç»ƒ ...
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **å®Œæ•´ç¤ºä¾‹**: [examples/agent_usage.py](examples/agent_usage.py)
- **è¯¦ç»†APIæ–‡æ¡£**: [docs/AGENT_API.md](docs/AGENT_API.md)
- **è®­ç»ƒè„šæœ¬**: [scripts/train_dreamer.py](scripts/train_dreamer.py)
- **è¯„ä¼°è„šæœ¬**: [scripts/evaluate.py](scripts/evaluate.py)

---

## ğŸ”— ç›¸å…³é¡¹ç›®å¯¹æ¯”

| é¡¹ç›® | æ¥å£è®¾è®¡ | RLæ”¯æŒ | ILæ”¯æŒ |
|------|---------|--------|--------|
| **Curi1 World** | BaseAgent | DreamerV4 | BC |
| **LeRobot** | PreTrainedPolicy | SAC, TDMPC | ACT, Diffusion |
| **UnifoLM-WMA** | æ— ç»Ÿä¸€æ¥å£ | âŒ | Diffusion Policy |

**Curi1 World çš„ä¼˜åŠ¿ï¼š**
- âœ… ç»Ÿä¸€çš„ RL + IL æ¥å£
- âœ… åŸºäºä¸–ç•Œæ¨¡å‹çš„é«˜æ•ˆRL
- âœ… ç®€å•æ˜“ç”¨çš„ BC å®ç°
- âœ… å·¥å‚æ¨¡å¼çµæ´»åˆ›å»º
